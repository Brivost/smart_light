{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56652ba",
   "metadata": {},
   "source": [
    "# Fully Connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7c958da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x110f8dad0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "\n",
    "th.backends.quantized.engine = \"qnnpack\"  # for ARM CPU\n",
    "th.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a239e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(th.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = th.ao.quantization.QuantStub()\n",
    "        self.fc = th.nn.Linear(3, 2, bias=False)\n",
    "        # self.conv1 = th.nn.Conv2d(1, 2, 3, 1, 1, 1, bias=True)\n",
    "        # self.conv2 = th.nn.Conv2d(2, 1, 3, 1, 1, 1, bias=False)\n",
    "        self.dequant = th.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.fc(x)\n",
    "        # x = self.conv2(x)\n",
    "        # x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7af16c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/ymbq97dx4cn7_63wk1jxfyzh0000gn/T/ipykernel_47318/3952365199.py:21: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  pm = th.ao.quantization.prepare_qat(m)\n",
      "/var/folders/pd/ymbq97dx4cn7_63wk1jxfyzh0000gn/T/ipykernel_47318/3952365199.py:27: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  qm = th.ao.quantization.convert(pm.eval())\n"
     ]
    }
   ],
   "source": [
    "m = Model()\n",
    "\n",
    "m.qconfig = th.ao.quantization.QConfig(\n",
    "    activation=th.ao.quantization.MovingAverageMinMaxObserver.with_args(\n",
    "        quant_min=-128,\n",
    "        quant_max=127,\n",
    "        dtype=th.qint8,\n",
    "        qscheme=th.per_tensor_symmetric,\n",
    "        reduce_range=False,\n",
    "    ),\n",
    "    weight=th.ao.quantization.MovingAverageMinMaxObserver.with_args(\n",
    "        quant_min=-128,\n",
    "        quant_max=127,\n",
    "        dtype=th.qint8,\n",
    "        qscheme=th.per_tensor_symmetric,\n",
    "        reduce_range=False,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Prepare\n",
    "pm = th.ao.quantization.prepare_qat(m)\n",
    "\n",
    "# Train\n",
    "pm(th.rand(32, 3))\n",
    "\n",
    "# Convert\n",
    "qm = th.ao.quantization.convert(pm.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eeb08b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6651, 0.8751, 0.3390]]),\n",
       " tensor([[ 85, 112,  43]], dtype=torch.int8),\n",
       " tensor([[ 16, -78]], dtype=torch.int8),\n",
       " tensor([[  52,    8,  -65],\n",
       "         [  63, -127,  -79]], dtype=torch.int8),\n",
       " tensor([0., 0.]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "x = th.rand(1, 3)\n",
    "xq = qm.quant(x)\n",
    "yq = qm(x)\n",
    "x, xq.int_repr(), yq.int_repr(), qm.fc.weight().int_repr(), qm.fc.bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c256330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
